---
title: MDPs Part 1 - The Markov Process (MP)
desc: This article is the first of a three-part series broken down into the core components of MDPs, focusing on Markov Processes (MPs).
img_url: rl-mp.png
date: 05/02/2022
tag: rl
topic: rl_intro
type: article
permalink: /blog/rl_intro/:title
layout: single-page
---
<p>Markov Decision Processes (MDPs) are used to describe environments that use a decision-maker (agent) to interact with a fully observable environment. Formally, almost all Reinforcement Learning (RL) problems are MDPs. Some examples include optimal control problems that focus on continuous MDPs, partially observable problems, specifically one's convertible to MDPs, or bandit problems consisting of single state MDPs.</p>
<p>This article is the first of a three-part series broken down into the core components of MDPs, focusing on Markov Processes (MPs). The second follows an extension of MPs with rewards, turning them into Markov Reward Processes (MRPs), found <a href="/blog/rl_intro/mdp_rewards">here</a>. The third, and last article, adds actions to those MRPs to create and complete the MDP, located <a href="/blog/rl_intro/mdp_decisions">here</a>.</p>

<h2>The Markov Process (MP)</h2>
<p>One of the simplest and fundamental components of MDPs is the MP, also known as a Markov chain. Markov chains, named after Andrey Markov, are mathematical systems consisting of observable states (a situation or set of values). The states within these systems can switch from one to another based on some specified probabilistic rules. Typically, a Markov process is a memoryless random process that consists of a sequence of random states, \(S_1,S_2,\cdots\), with the Markov property. Represented as a tuple of two elements, \(〈S,P〉\), where:</p>
<ul>
    <li>\(S\) - a finite set of states.</li>
    <li>\(\mathcal{P}\) - a state transition probability matrix.</li>
</ul>
<p>All possible states for a system form a set called the <span class="med">state space</span>. In MPs, this set of states must be finite (but can be extremely large) and uses a sequence of <span class="med">state observations</span>, also known as a <span class="med">chain</span>. For example, we have two states for the weather in a city, sunny <span style="font-style: italic;">(S)</span> or rainy <span style="font-style: italic;">(R)</span>, which make up a state space. When observing these states, a sequence of observations is formed over time, creating a chain of states, such as \([S,R,S,S,R,…]\), known as <span class="med">history</span>.</p><p>
</p><p>For a system to be a MP, it needs to fulfil the <span class="med">Markov Property</span>, also known as a Markov State. The Markov Property explains that the future depends only on the present state and doesn't depend on its history. More formally:</p>
<p class="quote">"Markov states capture all the relevant information from the history."</p>
<p>In RL algorithms, statistically, if a state is Markov, then the probability of what happens next in an agent's environment, \(S_{(t+1)}\), depends only on the current state the agent is in, \(S_t\), and not on everything that came before it, denoted in equation 1.1.</p>
{%- include single_parts/equation.html text="\mathbb{P}[S_{t+1}|S_t] = \mathbb{P}[S_{t+1}|S_1, \cdots, S_t]" index="1.1" -%}

<h2>State Transition Matrix</h2>
<p>Every state within a Markov chain has at least two possible transitions (except for terminal states), one to itself and one to another state. Each transition has a probability assigned to it, where every states transition probabilities are stored within a <span class="med">state transitions matrix (STM)</span>, denoted by \(\mathcal{P}\). STMs are a square matrix of size \(N \times N\), where \(N\) is the number of states in a model (Markov chain), denoted in equation 2.1, and a single row (state) must sum to 1.</p>
{%- include single_parts/equation.html text="\mathcal{P} = \left[\begin{array}{ccc}
    \mathcal{P}_{11} & \cdots & \mathcal{P}_{1n} \\ \vdots &  &  \\ \mathcal{P}_{n1} & \cdots & \mathcal{P}_{nn} \end{array}\right]" index="2.1" -%}
<p>Each row contains the probability that the system transitions from one state to another (including itself), defined as the transition probability from all states \(s\) to all successor states \(s{'}\). Statistically, a Markov state \(s\) and successor state \(s{'}\) have a <span class="med">state transition probability</span> that the Markov state will turn into the successor state, denoted in equation 2.2.</p>
{%- include single_parts/equation.html text="\mathcal{P}_{ss{'}} = \mathbb{P}[S_{t+1} = s{'}|S_t = s]" index="2.2" -%}
<p>The state transition matrix provides a complete structure to the Markov problem, where given a state, it is easy to determine how likely we are to end up in another state. Using the example from earlier, the transition matrix for sunny/rainy could look like this:</p>
{%- include single_parts/table.html head=" , Sunny, Rainy" body="Sunny, 0.8, 0.2 | Rainy, 0.1, 0.9" label="Table 2.1. Weather example state transition matrix." bold_first_col=true -%}
<p>In table 1.1, if the day is sunny, there is a 90% chance that it will stay sunny the next day and a 20% chance it will be rainy. Similarly, if it is a rainy day, there is a 10% probability that the weather will be sunny the following day and a 90% probability it stays rainy.</p>
{%- include single_parts/image.html url="rl-mp.png" alt="MP example" label="Figure 2.1. Weather MP model example." -%}
<p>The weather example visualised in figure 1.1 contains nodes corresponding to system states and edges labelled with probabilities, representing a possible transition from state to state. Furthermore, if the probability of a transition is 0%, the corresponding lines are not drawn, and the nodes are displayed as squares, to represent a terminal state.</p>
<p>In practice, the exact transition matrix is rarely known. Instead, system state observations are divided into sample <span class="med">episodes</span>. For example:</p>
<ul>
    <li>Sunny → Sunny → Rainy → Rainy → Rainy → Sunny</li>
    <li>Sunny → Rainy → Rainy → Sunny</li>
</ul>
<p>Episodes play a critical role in RL algorithms and are discussed further within the next article in the series. To continue with this series, check out the second article on <a href="/blog/rl_intro/mdp_rewards">Markov Reward Processes (MRPs)</a>.</p>

<h2>References</h2>
<p>RL Course by David Silver (2015) - <a href="https://www.youtube.com/watch?v=lfHX2hHRMVQ&list=PLqYmG7hTraZBiG_XpjnPrSNw-1XQaM_gB">Lecture 2: Markov Decision Process</a></p>
<p>Book by Richard S. Sutton and Andrew G. Barto (2018) - <a href="https://mitpress.mit.edu/books/reinforcement-learning-second-edition">Reinforcement Learning: An Introduction, 2nd Edition</a></p>
<p>Book by Maxim Lapan (2020) - <a href="https://www.packtpub.com/product/deep-reinforcement-learning-hands-on-second-edition/9781838826994">Deep Reinforcement Learning Hands-On, Second Edition</a></p>
<p>RL Course by Udacity (2021) - <a href="https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893">Deep Reinforcement Learning Nanodegree</a></p>
<p>Article by Victor Powell and Lewis Lehe (2014) - <a href="https://setosa.io/ev/markov-chains/">Markov Chains: Explained Visually</a></p>
<p>Article by Mohammad Ashraf (2021) - <a href="https://becomesentient.com/markove-decision-processes/">Reinforcement Learning Demystified: Markov Decision Processes (Part 1)</a></p>

---
title: MDPs Part 2 - The Markov Reward Process (MRP)
desc: This article is the second of a three-part series broken down into the core components of MDPs, focusing on Markov Reward Processes (MRPs).
img_url: rl-mrp.png
date: 06/02/2022
tag: rl
topic: rl_intro
type: article
permalink: /blog/rl_intro/:title
layout: single-page
---
<p>This article is the second of a three-part series broken down into the core components of MDPs, focusing on Markov Reward Processes (MRPs). This one focuses on Markov Reward Processes (MRPs). If you missed the first article on Markov Processes (MPs), please find it <a href="/blog/rl_intro/mdp_process">here</a>. Alternatively, for the third article, on the final component of MDPs, click <a href="/blog/rl_intro/mdp_decisions">here</a>.</p>

<h2>Markov Reward Process (MRP)</h2>
<p>MRPs are an extension of MPs that contain two additional components: a reward function and a discount factor.</p>
<p>Formally, MRPs are a tuple \(〈S,\mathcal{P},\mathcal{R},\gamma〉\), where:</p>
<ul>
<li>\(S\) - a finite set of states.</li>
<li>\(\mathcal{P}\) - a state transition probability matrix, \(\begin{equation}\mathcal{P}_{ss{'}} = \mathbb{P}[S_{t+1} = s{'} \; | \; S_t = s]\end{equation}\)</li>
<li>\(\mathcal{R}\) - a reward function, \(\begin{equation} \mathcal{R}_s = \mathbb{E}[R_{t+1} \; | \; S_t = s]\end{equation}\)</li>
<li>\(\gamma\) - a discount rate, \(\gamma \in [0,1]\)</li>
</ul>
<p>Reward functions are used to explain how much immediate reward \(\mathcal{R}\) an agent receives within a given state \(S\).</p>

<h2>Return</h2>
<p><span class="med">Return (\(G_t\))</span> is the total reward over a given number of timesteps \(t\), represented in equation 2.1, where \(T\) is the final timestep. In general, this is the simplest case for return. The formula makes sense in applications which have a natural notion of final timestep, such as sequences that terminate, more formally known as <span class="med" style="font-style: italic;">episodic tasks</span>.</p>
{%- include single_parts/equation.html text="G_t = R_{t+1} + R_{t+2} + R_{t+3} + \cdots + R_T" index="2.1" -%}
{%- include single_parts/equation.html text="G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots = \sum_{k=0}^{\infty}{\gamma^k R_{t+k+1}}" index="2.2" -%}
<p>In many cases, RL environment interactions don't naturally break into identifiable episodes. Instead, they go on continuously without limit, formally known as <span class="med" style="font-style: italic;">continuing tasks</span>. The formula highlighted in 2.1 can be problematic for these types of tasks due to the finite limitation. MRPs include a discount rate within the return to accommodate this.</p>
<p>Overall, the <span class="med">discounted return</span> (equation 2.2) encourages agents to select actions to maximize the sum of the discounted rewards it receives over the future. In short, it is used to maximize the total expected discounted return over a given number of timesteps \(t\).</p>
<p>It does this by using a discount rate \(\gamma \), which determines the present value of future rewards. If \(\gamma = 0\), the agent is 'myopic' in that it is only concerned with maximizing immediate rewards. However, as \(\gamma \) approaches 1, the agent takes future rewards into account more strongly and becomes more farsighted.</p>
<p>Discounted return can be beneficial for these reasons:</p>
<ul>
    <li>It is mathematically convenient.</li>
    <li>It assists in avoiding infinite returns in cyclic MPs.</li>
    <li>It accounts for uncertainty about the future.</li>
    <li>It provides a better reflection of human/animal behaviour, as immediate rewards are typically more favoured.</li>
</ul>

<h2>Value Functions</h2>
<p>Almost all RL algorithms involve estimating <span class="med">value functions</span>. Denoted by \(v(s)\), these functions estimate how good it is for an agent to be in a given state \(s\). This notion of effectiveness relates to the future expected return. In MRPs, the value function is the expected return starting from state \(s\), outlined in equation 3.1.</p>
{%- include single_parts/equation.html text="v(s) = \mathbb{E}[G_t \; | \; S_t = s]" index="3.1" -%}
<p>The value functions purpose is to provide a clear indication of how much reward an agent receives starting at a given state until a terminal state is reached.</p>
<p>Using a slightly more complex example than the weather one, highlighted in figure 3.1, we can see how rewards work in a state transition graph. The model focuses on an office worker that contains a state space of the following states:</p>
<ul>
    <li><span class="med">Home</span> - not at the office</li>
    <li><span class="med">Computer</span> - working at the office</li>
    <li><span class="med">Coffee</span> - drinking coffee at the office</li>
    <li><span class="med">Chatting</span> - with colleagues at the office</li>
</ul>

{%- include single_parts/image.html url="rl-mrp.png" alt="MRP example" label="Figure 3.1. Office worker example." -%}

<p>The model shows that the office worker always starts his day from the <span class="med">home</span> state and begins his shift with <span class="med">coffee</span>. Additionally, the workday always ends in the <span class="med">home</span> state from the <span class="med">computer</span> state. There are no exceptions to these rules. With this in mind, the state transition matrix for the graph is as follows:</p>

{%- include single_parts/table.html head=" , Home, Coffee, Chat, Computer" body="Home, 0.6, 0.4, 0, 0 | Coffee, 0, 0.1, 0.7, 0.2 | Chat, 0, 0.2, 0.5, 0.3 | Computer, 0.2, 0.2, 0.1, 0.5" label="Table 3.1. Office worker state transition matrix." bold_first_col=true -%}

<p>Some example episodes may look like the following:</p>
<ul>
    <li>Home → Coffee → Coffee → Chat → Chat → Coffee → Computer → Computer → Home</li>
    <li>Home → Home → Coffee → Computer → Home</li>
    <li>Home → Coffee → Chat → Coffee → Computer → Home</li>
</ul>
<p>Using these example episodes, we can calculate their expected return. With a \(\gamma = 0.8\), we would have the following:</p>
<p>\(v_1\) = 1 - (1 * 0.8) + (0 * 0.64) - (2 * 0.51) + (0 * 0.41) + (3 * 0.33) + (5 * 0.26) + (2 * 0.21) = 1.89</p>
<p>\(v_1\) = 1 + (1 * 0.8) + (3 * 0.64) + (2 * 0.51) = 4.74</p>
<p>\(v_1\) = 1 + (0 * 0.8) + (0 * 0.64) + (3 * 0.51) + (2 * 0.41) = 3.35</p>

<h2>Bellman Equation</h2>
<p>The value function can be decomposed into two parts:</p>
<ul>
    <li>Immediate reward \(R_{t+1}\)</li>
    <li>And the discounted value of the successor state \(\gamma v(S_{t+1})\)</li>
</ul>
{%- include single_parts/equation.html text="\begin{aligned}[t] v(s) &= \mathbb{E}[G_t \; | \; S_t = s] \\[5pt] &= \mathbb{E}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots \; | \; S_t = s] \\[5pt] &= \mathbb{E}[R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \cdots) \; | \; S_t = s] \\[5pt] &= \mathbb{E}[R_{t+1} + \gamma G_{t+1} \; | \; S_t = s] \\[5pt] &= \mathbb{E}[R_{t+1} + \gamma v(S_{t+1}) \; | \; S_t = s] \end{aligned}" index="4.1" -%}
<p>Unrolling the value function (equation 4.1), we get what is known as the Bellman equation, which expresses a relationship between the value of a state and the values of its successor state. Backup diagrams (figure 4.1) are commonly used in conjunction with the Bellman equation, visualising the idea of looking ahead from one state to a possible successor state and to provide graphical summaries of RL algorithms.</p>

{%- include single_parts/image.html url="bellman-backup.png" alt="Bellman backup diagram" label="Figure 4.1. Bellman backup diagram." -%}

<p>Each circle within figure 4.1 represents a state. Starting from state \(s\), the root node at the top, the agent could move to several next states \(s{'}\), and receive a reward \(r\). The value of the state \(s\) is the reward received upon leaving that state, plus a discounted average over the possible successor states, where the value for each of these successor states gets multiplied by the probability that the agent enters that successor state.</p>
<p>The Bellman equation averages over all the state possibilities, weighting each by its probability of occurring and explains that the value of the start state must equal the (discounted) value of the expected next state, plus the reward expected along the way. It can be expressed using matrices, where \(v\) represents a column vector within one entry per state, such that:</p>
{%- include single_parts/equation.html text="v = \mathcal{R} + \gamma \mathcal{P} v" index="4.2" -%}
{%- include single_parts/equation.html text="\left[\begin{array}{c} v(1) \\ \vdots \\ v(n) \end{array}\right] = \left[\begin{array}{c} \mathcal{R}_1 \\ \vdots \\ \mathcal{R}_n \end{array}\right] + \gamma \left[\begin{array}{ccc} \mathcal{P}_{11} & \cdots & \mathcal{P}_{1n} \\ \vdots &  &  \\ \mathcal{P}_{n1} & \cdots & \mathcal{P}_{nn} \end{array}\right] \left[\begin{array}{c} v(1) \\ \vdots \\ v(n) \end{array} \right]" index="4.3" -%}
<p>Equation 4.2 explains that the value function \(v\) for each state is equal to the reward function \(\mathcal{R}\) plus gamma \(\gamma \) multiplied by the dot product of the state transition probability matrix \(\mathcal{P}\) and the initial value function \(v\), creating a new value function using the Bellman equation.</p>
<p>Interestingly, the Bellman equation is linear, allowing it to be solved directly, as seen in equation 4.4, where \(I\) represents an identity matrix. The computational complexity of this equation is \(O(n^3)\) for n states, preventing it from being a practical solution with large MRPs and making it only a suitable solution to small MRPs.</p>
{%- include single_parts/equation.html text="\begin{aligned}[t] v &= \mathcal{R} + \gamma \mathcal{P} v \\[5pt] (I - \gamma \mathcal{P}) &= \mathcal{R} \\[5pt] v &= (I - \gamma \mathcal{P})^{-1} \mathcal{R} \end{aligned}" index="4.4" -%}
<p>Some iterative methods for large MRPs include:</p>
<ul>
    <li>Dynamic programming</li>
    <li>Monte-Carlo evaluation</li>
    <li>Temporal-difference learning</li>
</ul>
<p>Now that we have an understanding of MRPs, we can add the final component: decisions (actions), discussed in the third and final article <a href="/blog/rl_intro/mdp_decisions">Markov Decision Processes (MDPs)</a>.</p>

<h2>References</h2>
<p>RL Course by David Silver (2015) - <a href="https://www.youtube.com/watch?v=lfHX2hHRMVQ&list=PLqYmG7hTraZBiG_XpjnPrSNw-1XQaM_gB">Lecture 2: Markov Decision Process</a></p>
<p>Book by Richard S. Sutton and Andrew G. Barto (2018) - <a href="https://mitpress.mit.edu/books/reinforcement-learning-second-edition">Reinforcement Learning: An Introduction, 2nd Edition</a></p>
<p>Book by Maxim Lapan (2020) - <a href="https://www.packtpub.com/product/deep-reinforcement-learning-hands-on-second-edition/9781838826994">Deep Reinforcement Learning Hands-On, Second Edition</a></p>
<p>RL Course by Udacity (2021) - <a href="https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893">Deep Reinforcement Learning Nanodegree</a></p>
<p>Article by Mohammad Ashraf (2021) - <a href="https://becomesentient.com/markove-decision-processes/">Reinforcement Learning Demystified: Markov Decision Processes (Part 1)</a></p>
<p>Article by Blackburn (2019) - <a href="https://towardsdatascience.com/reinforcement-learning-markov-decision-process-part-2-96837c936ec3">Reinforcement Learning: Bellman Equation and Optimality (Part 2)</a></p>

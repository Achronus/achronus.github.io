---
title: Linear Regression
desc: Linear regression is a regression analysis technique used in predictive modelling that identifies the relationship between two types of variables independent and dependent. 
img_url: linear-reg.jpg
date: 07/02/2022
tag: supervised
topic: regression
type: article
permalink: /blog/regression/:title
layout: single-page
---
<p>Linear regression is a regression analysis technique used in predictive modelling that identifies the relationship between two types of variables: independent and dependent. Dependent variables (output) are a single feature that defines the main factor to understand or predict. Independent variables are one or more features (inputs) that we suspect impact the dependent variable, where each variable represents a vector (column) of values.</p>

<h2>Description</h2>
<p>The algorithm focuses on a regression line (line of best fit) that represents their relationship. The line becomes optimal when the total prediction error (distance) between the data points and the line is minimised. Equations 1.1 and 1.2 define a simple linear and multi-linear regression, respectively. Simple linear regression has one independent variable, while multi-linear regression has many.</p>
{%- include single_parts/equation.html text="\hat{y} = b_0 + b_1 x + \epsilon" index="1.1" -%}
{%- include single_parts/equation.html text="\hat{y} = b_0 + b_1 x_1 + b_2 x_2 + \cdots + b_n x_n + \epsilon" index="1.2" -%}
<p>In both equations (1.1 and 1.2), \(\hat{y}\) is the predicted value, \(b_0\) is the y-intercept, \(x\) is the independent variable, \(b_1\) is the slope (coefficient), and \(\epsilon\) is the residual or error term. Equation 1.2 contains numerous independent variables \((x_1, x_2, \cdots, x_n)\) with respective coefficients \((b_1, b_2, \cdots, b_n)\).</p>

<h2>Components</h2>
<h3>Y-Intercept</h3>
{%- include single_parts/equation.html text="b_0 = \bar{y} - b_1 \bar{x}_1 - b_2 \bar{x}_2 - \cdots b_n \bar{x}_n" index="1.3" -%}
<p>The y-intercept (slope), seen in equation 1.3, is calculated by subtracting the dependent variable's mean \((\bar{y})\) from every coefficient multiplied by its respective independent variable's mean.</p>
        
<h3>Ordinary Least Squares (OLS)</h3>
<p>The most common method for estimating the coefficient uses OLS, which minimises the sum of squared differences between the observed (independent) and predicted (dependent) variables. OLS has two forms, equation 1.4 represents the one for a simple linear regression, and equation 1.5 is for multi-linear regression.</p>
{%- include single_parts/equation.html text="b_1 = \dfrac{\sum\limits_{i=1}^N(x_i - \bar{x})(y_i - \bar{y})}{\sum\limits_{i=1}^N(x_i - \bar{x})^2}" index="1.4" -%}
<p>In equation 1.4, \(x\) is the independent variable, \(\bar{x}\) is the mean of the independent variable, \(y\) is the dependent variable, and \(y\) is the mean of the dependent variable. The numerator has two parts:</p>
<ol>
    <li>\((x_i - \bar{x})\) - the subtraction of each independent variable feature from its mean</li>
    <li>\((y_i - \bar{y})\) - the subtraction of each dependent variable feature from its mean</li>
</ol>
<p>We multiply both parts and repeat for all features, subsequently adding them together. The denominator focuses on the first part of the numerator, squaring the result, and following a similar process. We then divide the numerator by the denominator to create the coefficient.</p>
{%- include single_parts/equation.html text="b_n = (X^TX)^{-1} X^Ty" index="1.5" -%}
<p>Equation 1.5 takes the dot product of an inverted gram matrix \((X^T X)^{-1}\) and a moment matrix \(X^Ty\). Where \(X\) corresponds to a matrix of input values and \(X^T\) is its transpose. The first column of the gram matrix must contain ones, while the remaining is actual data (independent variables). The moment matrix is a unique symmetric square matrix containing monomials (polynomial/power product values) that takes the dot product of the transpose of the input values by the dependent variable.</p>

<h3>Error Term/Residual</h3>
<p>Error terms use a loss (cost) function to measure the accuracy between the model's predicted values and their actual values. Different loss functions provide distinct benefits for varying use cases. Details on the regression evaluation metrics are found <a href="/blog/regression/reg_loss_funcs">here</a>.</p>
<p>The regression line is an estimate based on the available data that depends on the error term to describe its credibility - the larger the error term, the more likely the independent variables do not affect the dependent variable.</p>

<h2>When To Use It</h2>
<p>In practice, we use linear regression to:</p>
<ol>
    <li>Explain an event that we want to understand (For example, why did the customer service calls drop last month?)</li>
    <li>Predict things about the future (For example, what will our sales look like over the next six months?)</li>
    <li>Or, to decide what to do (For example, should we go with this promotion or a different one?)</li>
</ol>
<p>A word of caution: correlation does not mean causation. Regression helps identify relationships, but it does not mean cause and effect. Remember, it strictly focuses on factors that are suspected to impact the dependent variable.</p>
<p>For example, imagine you are comparing sales with rainy days. Using regression techniques, we can identify the earning potential for 6 rainy days, such that for every additional rainy day, you make an average of 3 more sales.</p>

<h2>References</h2>
<p>Article by Amy Gallo (2015) - <a href="https://hbr.org/2015/11/a-refresher-on-regression-analysis">A Refresher on Regression Analysis</a></p>
<p>Article by Saishruthi Swaminathan (2018) - <a href="https://towardsdatascience.com/linear-regression-detailed-view-ea73175f6e86">Linear Regression - Detailed View</a></p>
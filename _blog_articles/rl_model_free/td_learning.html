---
title: TD Prediction & TD(\(\lambda\))
desc: A form of policy evaluation that focuses on estimating value functions of unknown MDPs, learning from incomplete episodes of experience.
img_url: td-prediction.png
date: 10/03/2022
tag: rl
topic: rl_model_free
type: article
permalink: /blog/rl_model_free/:title
layout: single-page
---
<p>In my previous article, <a href="/blog/rl_model_free/mc_prediction">Monte-Carlo Prediction</a>, we discussed MC Learning methods and how they estimate value functions of unknown Markov Decision Processes (MDPs). In this article, we discuss another type of method: Temporal-Difference (TD) Learning.</p>
<p>This article is the second the topic that focuses on model-free agents, arranged into individual articles for prediction and control algorithms. Model-Free Prediction (MFP) focuses on <span class="med">estimating</span> the value function of an <span class="med">unknown</span> MDP. While its counterpart, Model-Free Control (MFC), focuses on <span class="med">optimising</span> the <span class="med">unknown</span> MDPs value function.</p>

<h2>Temporal-Difference Learning</h2>
<p>Temporal-Difference (TD) methods learn directly from <span class="med">incomplete episodes of experience</span> by combining Monte-Carlo (MC) and dynamic programming (DP) concepts. During an episode, TD agents approximate the updated value function given its previous experience, allowing it to learn while it is still in the episode. Like MC, TD is a model-free method and uses a similar approach to DP by partially updating estimates based on other learned approximations in real-time (bootstrapping). Throughout the remainder of this article, we will discuss different TD methods, how they solve <span class="med">Temporal Credit Assignment</span> (TCA) problems, and the differences between TD, MC and DP.</p>

<h3>Temporal Credit Assignment Problem</h3>
<p>Before discussing the TCA problem, we need to understand the Credit Assignment (CA) problem. CA problems are tasks that require the correct actions taken that receive the most credit (reward). RL assists in solving this problem by using agents to find an optimum set of actions that maximizes reward.</p>
<p>The TCA problem is an extension of the CA problem that requires solutions across time, where an agent needs to find the best policy across multiple timesteps. Agents that solve these problems can learn in real-time by making updates to a policy during the progression of a task, which is extremely useful in real-world scenarios. In MC, agents have no awareness of time-critical events, such as hitting a moving target with a rifle or manoeuvring a car out of the way of oncoming traffic. With the added element of time or progression of events, TCA problems provide agents with the opportunity to learn the importance of such event timing.</p>

<h3>TD Prediction</h3>
<p>Like MC Prediction, TD Prediction's goal is to learn the value function \(v_\pi\) from experience under policy \(\pi\). The simplest algorithm is TD(0), which performs a modified incremental Every-Visit MC Prediction. The MC approach updates the estimated value \(V(S_t)\) towards the actual return \(G_t\) (equation 1.2.1). Comparatively, TD(0) updates the estimated value \(V(S_t)\) toward the estimated return \(R_{t+1} + \gamma V(S_{t+1})\), reflected in equation 1.2.2, and formally known as the TD update rule.</p>

{%- include single_parts/equation.html text="V(S_t) \leftarrow V(S_t) + \alpha \bigg( G_t - V(S_t) \bigg)" index="1.2.1" -%}
{%- include single_parts/equation.html text="V(S_t) \leftarrow V(S_t) + \alpha \bigg( R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \bigg)" index="1.2.2" -%}

<p>The estimated return consists of two parts:</p>
<ul>
    <li>The immediate return, \(R_{t+1}\)</li>
    <li>Plus, the discounted value of the next step, \(\gamma V(S_{t+1})\)</li>
</ul>
<p>Furthermore, equation 1.2.2 has two critical components have have specific terminology:</p>
<ul>
    <li>\(R_{t+1} + \gamma V(S_{t+1})\) is called the <span class="med">TD target</span>, which the agent aims to achieve.</li>
    <li>\(\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t)\) is called the <span class="med">TD error</span>. Representing the difference between the current and target estimates.</li>
</ul>
<p>TD methods are powerful because they update the value function after every timestep (real-time). Additionally, they can learn without a terminal outcome and work in continuing (non-terminating) environments. In comparison, MC methods require completing an entire episode before updating the value function and can only work within episodic (terminating) environments.</p>
<p>For example, imagine an agent driving a car, and it crashes into another one ahead of it. Using MC learning, the agent needs to wait until it crashes to receive a negative reward. However, with TD learning, the agent would receive a negative reward as it gets too close to the car and slow down before crashing.</p>

<h2>TD(\(\lambda\))</h2>
<p>TD(\(\lambda\)) is a popular algorithm that uses a mechanism called <span class="med">eligibility traces</span> (ETs), where \(\lambda\) is the eligibility trace. TD methods can be combined with ETs, such as Q-Learning or Sarsa, to obtain a general approach for learning more efficiently.</p>
<p>There are two ways to view ETs: theoretically and mechanistically. The theoretical view works as a bridge between TD and MC methods, producing a family of algorithms spanning from one-step TD to MC methods. Comparatively, the mechanistic view focuses on ETs as temporary records of an occurrence of an event, such as a state visit or action taken. Both have more formal names: the theoretical view is called <span class="med">forward-view</span>, and the mechanistic view is called <span class="med">backward-view</span>.</p>
<p>Throughout this section, we focus on the prediction problem and how ETs help with estimate value functions \(v_\pi\).

<h3>\(n\)-step</h3>
<p>\(n\)-step is one of the fundamental theoretical methods that produce a range of algorithms between one-step TD and MC. Recall that TD(0) performs a single backup on the next reward, and MC performs a backup for each state within the entire sequence of observed rewards until the end of the episode. By expanding TD(0), we can create two-step, three-step or even \(n\)-step backup methods, visualised in figure 2.1.1.</p>

{%- include single_parts/image.html url="td-n-step.png" alt="N-step TD comparsion" label="Figure 2.1.1. Temporal-Difference Learning look ahead variants." -%}

<p>To understand this mathematically, let's consider the following \(n\)-step returns for \(n = 1, 2, \infty\); seen in equation 2.1.1. In the equation, notice how additional steps are simple to implement and follow standard return methods.</p>

{%- include single_parts/equation.html text="\begin{aligned}[t] n &= 1 \; \; \; (TD) \quad \; G_t^{(1)} = R_{t+1} + \gamma V(S_{t+1}) \\ n &= 2 \; \qquad \qquad \; G_t^{(2)} = R_{t+1} + \gamma R_{t+2} + \gamma^2 V(S_{t+2}) \\ &\vdots \; \qquad \qquad \qquad \; \;  \vdots \\ n &= \infty \; \; (MC) \quad G_t^{(\infty)} = R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{T-1} R_T \end{aligned}" index="2.1.1" -%}

<p>Both MC (\(\infty\)) and TD methods use the traditional discounted expected return formula, summing \(n\)-steps of actual rewards. However, the TD methods have one minor addition - they add the estimated return of the \(n\)th step, accounting for the absence of the future returns. Equation 2.1.2 displays the complete formula for \(n\)-step TD returns.</p>

{%- include single_parts/equation.html text="G_t^{(n)} = R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1} R_{t+n} + \gamma^n V(S_{t+n})" index="2.1.2" -%}
{%- include single_parts/equation.html text="V(S_t) \leftarrow V(S_t) + \alpha \left( G_t^{(n)} - V(S_t) \right)" index="2.1.3" -%}

<p>Equation 2.1.3 express the \(n\)-step TD learning formula, where the TD target is \(G_t^{(n)}\), and the TD error is \(G_t^{(n)} - V(S_t)\). Unfortunately, \(n\)-step TD methods are inconvenient to implement. When computing \(n\)-step returns, agents must wait for \(n\) steps before observing the rewards and states. For large step sizes, this can prove to be problematic, especially when solving the control problem. Therefore, it is used strictly for theory to help understand similar methods.</p>

<h3>Forward-View TD(\(\lambda\))</h3>
<p>Moving onto more theoretical views of ETs, we can begin to understand what is computed by methods that use them. As seen in the previous section, \(n\)-step methods are one tool for obtaining returns, and on their own, they can prove computationally inefficient. Instead, we can combine multiple \(n\)-step methods and average the \(n\)-step returns, creating a new range of algorithms. For example, we perform backups towards a target using half a two-step return and half a four-step return, depicted in equation 2.2.1.</p>

{%- include single_parts/equation.html text="\frac{1}{2} G^{(2)} + \frac{1}{2} G^{(4)}" index="2.2.1" -%}

<p>Furthermore, we can combine any set of returns that we can average in the same way, even when using an infinite set. The only requirement is that the weights of the \(n\)-step components are positive values that sum to 1 (100%). The composite return creates an error reduction property similar to the individual \(n\)-step returns, providing guaranteed convergence.</p>
<p>Formally, backups that average component backups are called <span class="med">complex backups</span>. Backup diagrams for these consist of the component's backups with a horizontal line above and the weighting fractions below them. Figure 2.2.1 presents the backup diagram of the example mentioned above.</p>

{%- include single_parts/image.html url="complex-backup.png" alt="Complex backup diagram" label="Figure 2.2.1. A complex backup diagram containing half a two-step return and half a four-step return." -%}

<p>TD(\(\lambda\)) provides a method for averaging complex backups, which contains all the \(n\)-step backups, each weighted proportional to \(\lambda^{n-1}\), where \(\lambda∈[0,1]\), and normalized by a factor of \(1 - \lambda\), ensuring the sum of the weights equals 1 (see figure 2.2.2). The result provides a backup towards a return, formally called <span class="med">\(\lambda\)-return</span>, mathematically described in equation 2.2.2.</p>

{%- include single_parts/equation.html text="G_t^\lambda = (1 - \lambda) \sum\limits_{n=1}^\infty \lambda^{n-1} G_t^{(n)}" index="2.2.2" -%}

<p>Changing the value of lambda can provide different results. For example, when \(\lambda = 1\), performing backups according to the λ-return causes the same results as constant-\(\alpha\) MC. However, when \(\lambda = 0\), the \(\lambda\)-return acts as a one-step return, identical to TD(0).</p>

{%- include single_parts/image.html url="td-lambda.png" alt="TD Lambda" label="Figure 2.2.2. Backup diagrams for TD(\(\lambda\)), where \(\lambda\) enables different functionality. The diagram shows backup examples of one-step, two-step, three-step, and \(\infty\)-steps." -%}

<p>The TD(\(\lambda\)) algorithm uses geometric weighting to increase the computational efficiency by using a memoryless system that prevents the need to store or compute different values at each \(n\)-step return, enabling it to perform at the same computational cost as TD(0). To incorporate this, we replace the TD target \(G_t^{(n)}\) in equation 2.1.3, with the \(\lambda\)-return as the new method for performing backups, denoted by \(G_t^\lambda\), formally creating the forward-view update (equation 2.2.3).</p>

{%- include single_parts/equation.html text="V(S_t) \leftarrow V(S_t) + \alpha \bigg( G_t^\lambda - V(S_t) \bigg)" index="2.2.3" -%}

<p>Forward-view TD(\(\lambda\)) updates the value function towards the \(\lambda\)-return by looking into the future to compute \(G_t^\lambda\). During the process, when each state is visited, we look at the future rewards and determine the best approach for combining them. After updating the first state, we move on to the next, never returning to the previous one. Future states are viewed and processed repeatedly, but only once from each vantage point that precedes them (figure 2.2.3).</p>

{%- include single_parts/image.html url="forward-view.png" alt="Forward-view example" label="Figure 2.2.3. An example of the forward-view, where state updates are decided by looking at future rewards and states." -%}

<h3>Backward-View TD(\(\lambda\))</h3>
<p>Backward-view uses eligibility traces (ETs) as a way of storing occurrences of events, such as a state visit or action taken, and provide intuition to developing TD algorithms. The trace decides if the associated events are eligible for learning changes, bridging the gap between events and training information.</p>
<p>An ET for state \(s\) at time \(t\) is a random positive number, denoted \(E_t(s) \in \mathbb{R}^+\). During each step, the ETs of all non-visited states decay by \(\gamma \lambda\) (equation 2.3.1), where \(\gamma\) is the discount rate, and \(\lambda\) is a value between \([0,1]\), called a <span class="med">trace-decay parameter</span>.</p>

{%- include single_parts/equation.html text="E_t(s) = \gamma \lambda E_{t-1}(s)" index="2.3.1" -%}

<p>ETs are expandable to visited states at timestep \(t\), where the ET decays like normal but includes an addition of an <span class="med">indicator function</span>, denoted <span class="med">1</span> (equation 2.3.2). The indicator function accepts a state at timestep \(t\) as a parameter and returns 1 if it is visited or 0 otherwise.</p>

{%- include single_parts/equation.html text="E_t(s) = \gamma \lambda E_{t-1}(s) + \boldsymbol{1}(S_t = s)" index="2.3.2" -%}

<p>These types of eligibility trace have a formal name called an <span class="med">accumulating trace</span>. Accumulating traces accumulate each time the state is visited, then gradually decrease (fade) when it is no longer actively selected, illustrated in figure 2.3.1.</p>

{%- include single_parts/image.html url="accumulating-trace.png" alt="Accumulating trace example" label="Figure 2.3.1. An example of an accumulating trace, showing the increase and decrease of the trace as a state visit fluctuates." -%}

<p>Recently visited states are stored as a simple record, defined by \(\gamma \lambda\). Each stored trace (state) indicates the degree of <span class="med">eligibility</span> for experiencing learning changes when a reinforcing event occurs, or more accurately, a TD error. Equation 2.3.3 highlights an example of a one-step TD error. The equation takes an initial reward \(R_{t+1}\) and adds it to a discounted return of the agent's next state \(\gamma V(S_{t+1})\), subtracted by the current estimated return \(V(S_t)\).</p>

{%- include single_parts/equation.html text="\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t)" index="2.3.3" -%}

<p>Backward-view TD(\(\lambda\)) stores an ET for every state \(s\), then updates the estimated value function \(V(s)\) for each state in proportion to the TD-error \(\delta_t\) and eligibility trace \(E_t(s)\), mathematically represented in equation 2.3.4. The TD-error and ET are multiplied by \(\alpha\), a learning rate between \([0,1]\) that determines the agent learning speed. Formally, this equation represents the backward-view TD update.</p>

{%- include single_parts/equation.html text="V(s) \leftarrow V(s) + \alpha \delta_t E_t(s)" index="2.3.4" -%}

{%- include single_parts/image.html url="backward-view.png" alt="Backward view diagram" label="Figure 2.3.2. The backward view, where each update depends on the current TD error combined with eligibility traces of past events." -%}

<p>To better understand the backward view, let us examine different values of \(\lambda\). At the lowest value, 0, only the current state is updated, creating the same update behaviour as TD(0) (equation 2.3.5).</p>

{%- include single_parts/equation.html text="V(s) \leftarrow V(s) + \alpha \delta_t" index="2.3.5" -%}

<p>At the highest value, 1, we create the TD(1) algorithm, where credit given to earlier states deteriorates by \(\gamma\) per step, achieving similar behaviour to MC. Standard MC methods are limited to episodic tasks, but TD(1) significantly increases their range of applicability by extending the functionality to discounted continuing ones. Furthermore, recall from the previous article that MC methods only update their estimates at the end of an episode. TD(1) learns through an \(n\)-step approach, allowing immediate alteration of agent behaviour during the same episode, proving more valuable.</p>
<p>Concerning the performance of TD(\(\lambda\)) methods, accumulating traces have a single flaw. When \(\lambda &gt; 0.9\) and \(\alpha &gt; 0.5\), the algorithm becomes unstable. There are two variations of ETs that accommodate these limitations: Dutch and replacing traces. Each variation decays the traces of the non-visited states in the same way but differs when incrementing visited states.</p>

{%- include single_parts/image.html url="trace-variants.png" alt="Trace comparsion diagram" label="Figure 2.3.3. The three different kinds of traces in action. Accumulating traces add up each time a state is visited, whereas replacing traces reset to one, and Dutch traces perform an intermediate function depending on \(\alpha\). The diagram shows examples for \(\alpha = 0.5\) and a decay rate of \(\gamma \lambda = 0.8\) per step." -%}

<p>The first variation we will discuss is <span class="med">replacing trace</span>. Consider a state that has been visited and revisited before decaying to zero. With an accumulating variant, the revisit increments, achieving a value larger than 1. However, when using a replacing trace, it is instead reset to 1 (equation 2.3.6). In a rare cases of TD(1), this type of trace can closely reflect a first-visit MC method.</p>

{%- include single_parts/equation.html text="E_t(S_t) = 1" index="2.3.6" -%}

<p>In the second variation, <span class="med">Dutch trace</span>, we perform an intermediate between accumulating and replacing traces, depending on the step-size of \(\alpha\) (equation 2.3.7). As \(\alpha\) approaches zero, it becomes an accumulating trace, and if \(\alpha = 1\), it becomes a replacing trace.</p>

{%- include single_parts/equation.html text="E_t(S_t) = (1 - \alpha) \gamma \lambda E_{t-1}(S_t) + 1" index="2.3.7" -%}

<p>Both variants increase performance and provide more robust algorithms than accumulating traces. Dutch trace, in particular, can achieve performance that closely approximates the \(\lambda\)-return algorithm.</p>

<h2>References</h2>
<p>RL Course by David Silver (2015) - <a href="https://www.youtube.com/watch?v=PnHCvfgC_ZA&list=PLqYmG7hTraZBiG_XpjnPrSNw-1XQaM_gB">Lecture 4: Model-Free Prediction</a></p>
<p>Book by Richard S. Sutton and Andrew G. Barto (2018) - <a href="https://mitpress.mit.edu/books/reinforcement-learning-second-edition">Reinforcement Learning: An Introduction, 2nd Edition</a></p>
<p>Book by Micheal Lanham (2020) - <a href="https://www.packtpub.com/product/hands-on-reinforcement-learning-for-games/9781839214936">Hands-On Reinforcement Learning for Games</a></p>
<p>RL Course by Udacity (2021) - <a href="https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893">Deep Reinforcement Learning Nanodegree</a></p>

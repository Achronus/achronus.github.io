---
title: RL - Terminology Cheatsheet
desc: Reinforcement Learning (RL) is an expansive and growing subfield within Machine Learning with a variety of important terminology. 
img_url: rl-cheatsheet.jpg
date: 07/02/2022
tag: rl
topic: rl_intro
type: article
permalink: /blog/rl_intro/:title
layout: single-page
---
<p>Reinforcement Learning (RL) is an expansive and growing subfield within Machine Learning with a variety of important terminology. Throughout my RL journey, I often found myself looking for a cheatsheet of vocabulary that I could refer back to whenever needed. This article aims to do just that! It focuses on critical terminology, fundamental to becoming a RL engineer.</p>

<h2>Terminology</h2>
<p><span class="med">Agent</span>: an AI system or controller used to navigate a given environment.</p>
<p><span class="med">Action</span>: a command that an agent uses during each timestep of its environment, denoted \(A_t\).</p>
<p><span class="med">Environment</span>: a represention of a specific world, such as a game board.</p>
<p><span class="med">Fully Observable Environemnt</span>: allows agents to directly observe the environment state.</p>
<p><span class="med">Partially Observable Environment</span>: agents indirectly observe the environment, only viewing information related to the required task.</p>
<p><span class="med">Reward</span>: a positive or negative scalar feedback signal provided to the agent after it has interacted with its environment, denoted \(R_t\).</p>
<p><span class="med">Reward Hypothesis</span>: every agents goal is to select actions to maximise total future reward.</p>
<p><span class="med">State</span>: acts as a snapshot of the current environment, denoted \(S_t\).</p>
<p><span class="med">State Space</span>: a set of all possible states within a Markov Process.</p>
<p><span class="med">Environment State</span>: environment's private representation used to determine what observation/reward to pick next, denoted \(S_t^e\).</p>
<p><span class="med">Agent State</span>: agent's internal representation of the world around it. Captures exactly what the agent has seen and done so far and used to determine the agent's next action, denoted \(S_t^a\).</p>
<p><span class="med">Information/Markov State</span>: contains all useful information from the history, denoted \(O_t\).</p>
<p><span class="med">History</span>: a stored sequence of observations, actions, and rewards, representing a list of observable variables up to time \(t\).</p>
<p><span class="med">Policy</span>: represents the agent's brain, determining how it makes decisions to select its actions. Can be deterministic or stochastic.</p>
<p><span class="med">Determinstic Policy</span>: acts as a map from state to action.</p>
<p><span class="med">Stochasic Policy</span>: agents make random exploratory decisions to see more of the state space.</p>
<p><span class="med">Value Function</span>: predicts future reward used to evaluate the effectives of each future state.</p>
<p><span class="med">Model</span>: represents the agent's view of how the environment works, where the agent attempts to foresee what happens next.</p>
<p><span class="med">Model-Free</span>: agents that don't build a model of their enviroment or contain rewards and instead directly connect observations/states to actions.</p>
<p><span class="med">Model-Based</span>: agents that try to predict the next state and reward in its environments.</p>
<p><span class="med">Value-Based</span>: agents have a value function and no policy.</p>
<p><span class="med">Policy-Based</span>: agents have a policy and no value function.</p>
<p><span class="med">Actor-Critic</span>: agents use both a policy and a value function.</p>
<p><span class="med">Learning Problem</span>: agents are initially unaware of the environment around them and must become familiar with it through interactions.</p>
<p><span class="med">Planning Problem</span>: agents have a model of the environment and know all its rules, where they perform computations (without external interaction) to improve its policy.</p>
<p><span class="med">Exploration</span>: agents discover more about their environment through random actions, substituing immediate rewards for even greater ones.</p>
<p><span class="med">Exploitation</span>: agents focus on abusing the known information to maximise its immediate reward.</p>
<p><span class="med">Prediction Problem</span>: involves evaluating the performance of agents in their future states, given the current policy.</p>
<p><span class="med">Control Problem</span>: focuses on finding an optimal policy to gain the most future reward.</p>
<p><span class="med">Markov Process</span>: also known as a Markov Chain, are mathematical systems consisting of observable states that can switch from one state to another based on a set of probabilistic rules. Typically, represented as a tuple of two elements \(〈S, \mathcal{P}〉\), where \(S\) is a finite set of states and \(\mathcal{P}\) is a state transition matrix.</p>
<p><span class="med">Makrov Property</span>: also known as a Markov State, is used to capture all the relevant information from the history, where:</p>
{%- include single_parts/equation.html text="\mathbb{P}[S_{t+1} \; | \;S_t] = \mathbb{P}[S_{t+1} \; | \; S_1, \cdots, S_t]" index="1.1" -%}
<p><span class="med">Markov Reward Process</span>: or MRP for short, are an extension of Makrov Processes that contain two additional components: rewards and a discount factor. Forms a tuple of four components \(〈S, \mathcal{P}, \mathcal{R}, \gamma〉\), where \(S\) is a finite set of states, \(\mathcal{P}\) is a state transition matrix, \(\mathcal{R}\) is a reward function, and \(\gamma\) is a discount rate.</p>
<p><span class="med">Makrov Decision Process</span>: or MDP for short, are an extension of Markov Reward Processes (MRPs) that introduces decisions (actions). Uses the same tuple as MRPs with the addition of actions, denoted by \(〈S, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma〉\). MDPs are an environment which contains a finite set of states that are Markov, designed to describe the agent/environment interaction setting.</p>
<p><span class="med">State Transition Matrix</span>: a square matrix of size \(N \times N\), where \(N\) is the number of states in a model, that stores every Markov Chain's state probabilities, as a row, for each possible transition assigned to them. Denoted by \(\mathcal{P}\), where:</p>
{%- include single_parts/equation.html text="\mathcal{P}_{ss'} = \mathbb{P}[S_{t+1} = s' \; | \; S_t = s]" index="1.2" -%}
<p><span class="med">Reward Function</span>: a mathematical function used to explain how much immediate reward \(R\) an agent receives within a given state \(S\), where:</p>
{%- include single_parts/equation.html text="\mathcal{R}_s = \mathbb{E}[R_{t+1} \; | S_t = s]" index="1.3" -%}
<p><span class="med">Return</span>: denoted by \(G_t\), representing the total reward over a given number of timesteps \(t\), with \(T\) as the final timestep. Calculated by combining multiple rewards together up to timestep \(t\). Typically used in episodic tasks, where:</p>
{%- include single_parts/equation.html text="G_t = R_{t+1} + R_{t+2} + R_{t+3} + \cdots + R_T" index="1.4" -%}
<p><span class="med">Discounted Return</span>: an extension of return that includes the use of a discount factor that is used to maximize the total expected discounted return over a given number of timesteps \(t\). Typically used in continuing tasks, where:</p>
{%- include single_parts/equation.html text="G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots = \sum\limits^{\infty}_{k = 0} \gamma^k R_{t+k+1}" index="1.5" -%}
<p><span class="med">Episodic Tasks</span>: a finite number of states that an agent performs in that concludes in a terminal state.</p>    
<p><span class="med">Continuing Tasks</span>: a never-ending set of states that doesn't have a terminal state.</p>
<p><span class="med">Value Function</span>: a mathematical function that is used to estimate 'how good' it is for an agent to be in a given state \(s\) by calculating a future expected return.</p>
<p><span class="med">Optimal Value Function</span>: the best performing value function, denoted with a *, calculated through taking the maximum value function over all policies generated within an environment. Consists of two separate formulas for both the state-value and action-value functions.</p>
<p><span class="med">State-Value Function</span>: the same as a standard value function but includes a policy to enable agent decision-making. Formally defined as the expected return starting from state \(s\) and then following the policy \(\pi\), where:</p>
{%- include single_parts/equation.html text="v_\pi(s) = \mathbb{E}_\pi[G_t \; | \; S_t = s]" index="1.6" -%}
<p><span class="med">Optimal State-Value Function</span>: the best performing state-value function, where:</p>
{%- include single_parts/equation.html text="v_*(s) = \max\limits_\pi v_\pi(s)" index="1.7" -%}
<p><span class="med">Action-Value Function</span>: the expected return starting from state \(s\), while taking an action \(a\), and then following the policy \(\pi\), where:</p>
{%- include single_parts/equation.html text="q_\pi(s, a) = \mathbb{E}_\pi[G_t \; | \; S_t = s, A_t = a]" index="1.8" -%}
<p><span class="med">Optimal Action-Value Function</span>: the best performing action-value function, where:</p>
{%- include single_parts/equation.html text="q_*(s, a) = \max\limits_\pi q_\pi(s, a)" index="1.9" -%}
<p><span class="med">Policy</span>: denoted by \(\pi\), used to define the behaviour of RL agents and helps them to choose which actions to take within their environment. A policy can either be stochastic or determinstic.</p>
<p><span class="med">Optimal Policy</span>: presents the best behaviour an agent can have within an environment. These policies can be found by defining a partial ordering over all policies, denoted by \(\pi_*\). Can be found by maximising over \(q_*(s, a)\), where:</p>
{%- include single_parts/equation.html text="\pi_*(a|s) = \left\{
    \begin{array}{@{} l c @{}} 1 & if \; a = {\arg\max\limits_{a \in A}} \; q_*(s,a) \\ 0 & otherwise \end{array}\right." index="1.10" -%}
<p><span class="med">Stochastic Policy</span>: a probability distribution over a set of actions, used for every state, allowing agents to select actions within a state based on a probability for a given action, where:</p>
{%- include single_parts/equation.html text="\pi(a|s) = \mathbb{P}[A_t = a \; | \; S_t = s]" index="1.11" -%}
<p><span class="med">Determinstic Policy</span>: used to map states to actions, preventing uncertainty when taking actions, \(\pi(s) = a\).</p>
<p><span class="med">Bellman Equation</span>: a decompressed version of a value function, split into two parts: an immediate reward \(R_{t+1}\) and the discounted value of the successor state \(\gamma v(S_{t+1})\). The equation is used to express a relationship between the value of a state and the values of its successor state, used in conjunction with backup diagrams to visualise a one or two-step look ahead from one state to a possible successor state, where:</p>
{%- include single_parts/equation.html text="v(s) = \mathbb{E}[R_{t+1} + \gamma v(S_{t+1}) \; | \; S_t = s]" index="1.12" -%}
<p><span class="med">Bellman Expectation Equations</span>: a decomposition of the state-value \(v_\pi(s)\) and action-value \(q_\pi(s, a)\) functions, similar to the Bellman Equation, where:</p>
{%- include single_parts/equation.html text="v_\pi(s) = \mathbb{E}_\pi[R_{t+1} + \gamma v_\pi(S_{t+1}) \; | \; S_t = s]" index="1.13" -%}
{%- include single_parts/equation.html text="q_\pi(s, a) = \mathbb{E}_\pi[R_{t+1} + \gamma q_\pi(S_{t+1}, A_{t+1}) \; | \; S_t = s, A_t = a]" index="1.14" -%}
<p><span class="med">Bellman Optimality Equations</span>: similar to the Bellman Expectation Equations, but instead of taking the average, we take the maximum (best) action value that an agent can take, where:</p>
{%- include single_parts/equation.html text="v_*(s) = \max\limits_{a} \mathcal{R}^a_s + \gamma \sum\limits_{s' \in S} \mathcal{P}^a_{ss'} v_*(s')" index="1.15" -%}
{%- include single_parts/equation.html text="q_*(s, a) = \mathcal{R}^a_s + \gamma \sum\limits_{s' \in S} \mathcal{P}^a_{ss'} \max\limits_{a'} q_*(s', a')" index="1.16" -%}
<p><span class="med">Dynamic Programming</span>: or DP for short, is a method for solving complex problems by breaking them down into subproblems. The subproblems are then solved and combined to complete the whole complex problem. DP can be used to solves problems that have two properties: an optimal substructure and overlapping subproblems.</p>
<p><span class="med">Principle of Optimality Theorem</span>: A policy \(\pi(a|s)\) achieves the optimal value from state \(s\) when the value of that policy is the optimal value function \(v_\pi(s) = v_*(s)\). However, this can only be true if any state \(s'\) is reachable from \(s\) when the policy \(\pi\) followed achieves the optimal value from state \(s'\), mathematically: \(v_\pi(s') = v_*(s')\).</p>
<p><span class="med">Optimal Substructure</span>: explains that complex problems are decomposable into multiple solvable subproblems, where the optimal solutions to those subproblems can determine the optimal solution to the overall problem. Requires a principle of optimality.</p>
<p><span class="med">Overlapping subproblems</span>: involves recursive subproblems that can be cached and reused to solve complex problems, typically through divide-and-conquer algorithms.</p>
<p><span class="med">Bellman's Curse of Dimensionality</span>: involves an expoential growth of the number of states, given the number of state variables \(n = |S|\), which can be very computational expensive for a single backup.</p>
<p><span class="med">Sample Backups</span>: used for large MDPs, which take a sample of rewards and transitions \(〈S, A, R, S'〉\), instead of a reward function \(\mathcal{R}\) and the MDPs transition dynamics \(\mathcal{P}\). Advantageous for breaking the curse of dimensionality, setting a constant cost of backups that are independent on \(n = |S|\). Also, in model-free algorithms, it removes the requirement to know the model of the environment.</p>
<p><span class="med">Policy Evaluation</span>: focuses on evaluating a fixed policy \(v_\pi\), and can be used to solve prediction problems. Given an MDP \(〈S, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma〉\) and a policy \(\pi\), the Bellman Expectation Equation is used and converted into an iterative update to use as a mechanism for evaluating the policy, and generate a true value function \(v_\pi\).</p>
<p><span class="med">Policy Improvement</span>: an approach used to find the best possible policy within a given MDP using a feedback process, where \(\pi' \ge \pi\). Uses the policy improvement theorem to identify the best policy \(q_\pi(s, \pi'(s)) \ge v_\pi(s)\).</p>
<p><span class="med">Policy Iteration</span>: a combination of policy evaluation and improvement to achieve an optimal policy \(\pi_*\) and satisfy the Bellman optimality equation. </p>
<p><span class="med">Random Policy</span>: a simple policy an agent follows to take any possible action at random. For example: a uniform random policy would consist of every action having the same probability chance. If there were four actions, each action would have a 25% chance of being taken.</p>
<p><span class="med">Greey Policy</span>: a policy used by agents to always take the best action an agent can take within a given state. This policy only reflects the short-term by perform a one-step look-ahead for \(v_\pi\). A greedy policy is created mathematically by:</p>
{%- include single_parts/equation.html text="\pi'(s) = arg \max\limits_{a \in A} \; q_\pi(s, a)" index="1.17" -%}
<p><span class="med">Value Iteration</span>: a special case of policy iteration, where policy evaluation is stopped after just one sweep of the state space. Uses an update rule version of the Bellman optimality equation. Intuitively, this type of algorithm starts at the terminal state of an MDP and works backwards to determine the optimal to that state to create the optimal policy \(\pi_*\). Mathematically:</p>
{%- include single_parts/equation.html text="v_{k+1}(s) = \max\limits_{a \in A} \left(\mathcal{R}^a_s + \gamma \sum\limits_{s' \in S} \mathcal{P}^a_{ss'} \; v_k(s') \right)" index="1.18" -%}
{%- include single_parts/equation.html text="v_{k+1} = \max\limits_{a \in A} \mathcal{R}^a + \gamma \mathcal{P}^a v_k" index="1.19" -%}
<p><span class="med">Synchronous Dynamic Programming</span>: the basic type of dynamic programming algorithms that require sweeps over an entire MDPs state space. Can be computationally expensive if the state space is large.</p>
<p><span class="med">Asynchronous Dynamic Programming</span>: a form of in-place iterative algorithms that perform in systematic sweeps of the state space. These types of algorithms backup the values of states individually in any order, proceeding with available state values. Can significantly reduce computation overhead, in comparsion to synchronous methods, and still guarantees convergence to optimal values. Uses three methods to assist in selecting states to update: in-place dynamic programming, prioritised sweeping, and real-time dynamic programming.</p>
<p><span class="med">In-place Dynamic Programming</span>: used to store a single copy of an MDPs value function for all states in the state space. Mathematically:</p>
{%- include single_parts/equation.html text="v(s) \rightarrow \max\limits_{a \in A} \left(\mathcal{R}^a_s + \gamma \sum\limits_{s' \in S} \mathcal{P}^a_{ss'} \; v(s') \right)" index="1.20" -%}
<p><span class="med">Prioritised Sweeping</span>: provides a measure of importance for each state within a state space. Uses the magnitude of the Bellman error to guide state selection by utilising a priority queue that determines which states are more important than others. Mathematically:</p>
{%- include single_parts/equation.html text="\left| \max\limits_{a \in A} \left(\mathcal{R}^a_s + \gamma \sum\limits_{s' \in S} \mathcal{P}^a_{ss'} \; v(s') \right) - v(s) \right|" index="1.21" -%}
<p><span class="med">Real-time Dynamic Programming</span>: a method that focuses on choosing states relevant to the agent (ones it has visited) to solve a given MDP. Works in conjunction with an iterative DP algorithm running to receive live samples of information based on the agent's experiences. The experiences are then used to guide the selection of states. After each time-step, we receive \(S_t, A_t, R_{t+1}\). When backing up the state \(S_t\), combined with an in-place calculation, we can identify the most relevant state. Mathematically:</p>
{%- include single_parts/equation.html text="v(S_t) \rightarrow \max\limits_{a \in A} \left(\mathcal{R}^a_{S_t} + \gamma \sum\limits_{s' \in S} \mathcal{P}^a_{S_t s'} \; v(s') \right)" index="1.22" -%}
